{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94597618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from 'aurora_location_model.pkl' …\n",
      "Model loaded OK.\n",
      "Connecting to the database …\n",
      "Loaded 207 hexagons for Lviv.\n",
      "Predicting probabilities …\n",
      "Prediction finished.\n",
      "Building interactive map …\n",
      "\n",
      "Top 10 recommended locations (predicted, no existing shop):\n",
      "              cell  aurora_probability\n",
      "0  881e7689abfffff            0.995856\n",
      "1  881e768839fffff            0.994050\n",
      "2  881e768915fffff            0.988771\n",
      "3  881e7689d1fffff            0.982486\n",
      "4  881e768957fffff            0.954386\n",
      "5  881e7689c9fffff            0.947204\n",
      "6  881e768811fffff            0.947133\n",
      "7  881e7689edfffff            0.937634\n",
      "8  881e7689c1fffff            0.931176\n",
      "9  881e768911fffff            0.928864\n",
      "\n",
      "✅  Map saved to 'lviv_aurora_prediction_map.html'. Open the file in a browser to explore!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DB_CONFIG = {\n",
    "    'host': 'localhost',\n",
    "    'port': 5432,\n",
    "    'database': 'osm',\n",
    "    'user': 'postgres',\n",
    "    'password': 'postpass'\n",
    "}\n",
    "\n",
    "conn_string = f\"postgresql://{DB_CONFIG['user']}:{DB_CONFIG['password']}@{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['database']}\"\n",
    "\n",
    "# Feature columns (excluding id, cell, has_aurora)\n",
    "FEATURE_COLUMNS = [\n",
    "    'bench_count', 'cafe_count', 'pharmacy_count', 'waste_disposal_count', \n",
    "    'atm_count', 'post_office_count', 'bank_count', 'restaurant_count', \n",
    "    'waste_basket_count', 'fuel_count', 'shelter_count', 'toilets_count', \n",
    "    'fast_food_count', 'place_of_worship_count', 'bicycle_parking_count', \n",
    "    'parking_count', 'bar_count', 'dentist_count', 'drinking_water_count', \n",
    "    'clinic_count', 'car_wash_count', 'payment_terminal_count', 'recycling_count', \n",
    "    'library_count', 'school_count', 'community_centre_count', 'vending_machine_count', \n",
    "    'pub_count', 'bureau_de_change_count', 'doctors_count', 'convenience_count', \n",
    "    'clothes_count', 'supermarket_count', 'hairdresser_count', 'yes_count', \n",
    "    'car_repair_count', 'beauty_count', 'hardware_count', 'alcohol_count', \n",
    "    'car_parts_count', 'butcher_count', 'chemist_count', 'bakery_count', \n",
    "    'mobile_phone_count', 'electronics_count', 'doityourself_count', \n",
    "    'furniture_count', 'florist_count', 'kiosk_count', 'pawnbroker_count', \n",
    "    'pet_count', 'shoes_count', 'confectionery_count', 'optician_count', \n",
    "    'cosmetics_count', 'jewelry_count', 'general_count', 'travel_agency_count', \n",
    "    'variety_store_count', 'greengrocer_count', 'atb_count', 'novus_count', \n",
    "    'eko_market_count', 'fora_count'\n",
    "]\n",
    "\n",
    "def load_data(conn_string):\n",
    "    \"\"\"Load data from PostgreSQL database\"\"\"\n",
    "    engine = create_engine(conn_string)\n",
    "    \n",
    "    # Load Kyiv data\n",
    "    query_kyiv = \"\"\"\n",
    "    SELECT * FROM osm_loc_alike_kyiv\n",
    "    WHERE boundary_effect = 0  -- Filter out locations very close to existing shops\n",
    "    \"\"\"\n",
    "    df_kyiv = pd.read_sql(query_kyiv, engine)\n",
    "    \n",
    "    # Load Lviv data\n",
    "    query_lviv = \"\"\"\n",
    "    SELECT * FROM osm_loc_alike_lviv\n",
    "    WHERE boundary_effect = 0  -- Filter out locations very close to existing shops\n",
    "    \"\"\"\n",
    "    df_lviv = pd.read_sql(query_lviv, engine)\n",
    "    \n",
    "    print(f\"Kyiv data shape (after boundary filter): {df_kyiv.shape}\")\n",
    "    print(f\"Lviv data shape (after boundary filter): {df_lviv.shape}\")\n",
    "    print(f\"Aurora shops in Kyiv: {df_kyiv['has_aurora'].sum()}\")\n",
    "    print(f\"Aurora shops in Lviv: {df_lviv['has_aurora'].sum()}\")\n",
    "    \n",
    "    return df_kyiv, df_lviv\n",
    "\n",
    "def explore_data(df, city_name):\n",
    "    \"\"\"Explore data distribution and patterns\"\"\"\n",
    "    print(f\"\\n{city_name} Data Exploration:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Class distribution\n",
    "    print(f\"Class distribution:\\n{df['has_aurora'].value_counts()}\")\n",
    "    print(f\"Class balance: {df['has_aurora'].sum() / len(df) * 100:.2f}% positive\")\n",
    "    \n",
    "    # Missing values\n",
    "    missing = df[FEATURE_COLUMNS].isnull().sum()\n",
    "    if missing.sum() > 0:\n",
    "        print(f\"\\nMissing values:\\n{missing[missing > 0]}\")\n",
    "    \n",
    "    # Feature statistics for shops vs non-shops\n",
    "    shop_mask = df['has_aurora'] == 1\n",
    "    \n",
    "    df.drop('aurora_count', axis=1, inplace=True)\n",
    "    \n",
    "    # Select top features by difference\n",
    "    feature_diffs = {}\n",
    "    for col in FEATURE_COLUMNS:\n",
    "        if col in df.columns:\n",
    "            shop_mean = df.loc[shop_mask, col].mean()\n",
    "            no_shop_mean = df.loc[~shop_mask, col].mean()\n",
    "            if no_shop_mean > 0:\n",
    "                feature_diffs[col] = abs(shop_mean - no_shop_mean) / no_shop_mean\n",
    "    \n",
    "    top_features = sorted(feature_diffs.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    print(f\"\\nTop differentiating features:\")\n",
    "    for feat, diff in top_features:\n",
    "        print(f\"  {feat}: {diff*100:.1f}% difference\")\n",
    "\n",
    "def prepare_features(df, feature_cols):\n",
    "    \"\"\"Prepare feature matrix and handle missing values\"\"\"\n",
    "    # Get available features\n",
    "    available_features = [col for col in feature_cols if col in df.columns]\n",
    "    \n",
    "    X = df[available_features].copy()\n",
    "    y = df['has_aurora'].copy()\n",
    "    \n",
    "    # Handle missing values\n",
    "    X = X.fillna(0)\n",
    "    \n",
    "    return X, y, available_features\n",
    "\n",
    "def train_models(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Train multiple models and compare performance\"\"\"\n",
    "    models = {\n",
    "        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced'),\n",
    "        'XGBoost': xgb.XGBClassifier(n_estimators=100, random_state=42, scale_pos_weight=len(y_train)/y_train.sum()),\n",
    "        'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "        'Logistic Regression': LogisticRegression(random_state=42, class_weight='balanced', max_iter=1000),\n",
    "        'SVM': SVC(kernel='rbf', probability=True, random_state=42, class_weight='balanced'),\n",
    "        'KNN': KNeighborsClassifier(n_neighbors=5)\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nTraining {name}...\")\n",
    "        \n",
    "        # Standardize features for some models\n",
    "        if name in ['Logistic Regression', 'SVM', 'KNN']:\n",
    "            scaler = StandardScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            y_pred = model.predict(X_test_scaled)\n",
    "            y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "        else:\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "        \n",
    "        results[name] = {\n",
    "            'model': model,\n",
    "            'predictions': y_pred,\n",
    "            'probabilities': y_pred_proba,\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred),\n",
    "            'recall': recall_score(y_test, y_pred),\n",
    "            'f1': f1_score(y_test, y_pred),\n",
    "            'auc': roc_auc_score(y_test, y_pred_proba) if len(np.unique(y_test)) > 1 else 0\n",
    "        }\n",
    "        \n",
    "        print(f\"  Accuracy: {results[name]['accuracy']:.3f}\")\n",
    "        print(f\"  Precision: {results[name]['precision']:.3f}\")\n",
    "        print(f\"  Recall: {results[name]['recall']:.3f}\")\n",
    "        print(f\"  F1-Score: {results[name]['f1']:.3f}\")\n",
    "        print(f\"  AUC: {results[name]['auc']:.3f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def plot_model_comparison(results):\n",
    "    \"\"\"Plot model performance comparison\"\"\"\n",
    "    metrics = ['accuracy', 'precision', 'recall', 'f1', 'auc']\n",
    "    model_names = list(results.keys())\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    x = np.arange(len(model_names))\n",
    "    width = 0.15\n",
    "    \n",
    "    for i, metric in enumerate(metrics):\n",
    "        values = [results[model][metric] for model in model_names]\n",
    "        ax.bar(x + i*width, values, width, label=metric.capitalize())\n",
    "    \n",
    "    ax.set_xlabel('Models')\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.set_title('Model Performance Comparison')\n",
    "    ax.set_xticks(x + width * 2)\n",
    "    ax.set_xticklabels(model_names, rotation=45)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def analyze_feature_importance(model, feature_names, top_n=20):\n",
    "    \"\"\"Analyze and plot feature importance\"\"\"\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importances = model.feature_importances_\n",
    "    elif hasattr(model, 'coef_'):\n",
    "        importances = np.abs(model.coef_[0])\n",
    "    else:\n",
    "        return\n",
    "    \n",
    "    # Sort features by importance\n",
    "    indices = np.argsort(importances)[::-1][:top_n]\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.title(\"Top Feature Importances\")\n",
    "    plt.barh(range(len(indices)), importances[indices])\n",
    "    plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
    "    plt.xlabel(\"Importance\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print top features\n",
    "    print(f\"\\nTop {top_n} most important features:\")\n",
    "    for i in indices:\n",
    "        print(f\"  {feature_names[i]}: {importances[i]:.4f}\")\n",
    "\n",
    "def predict_new_locations(model, df, feature_cols, top_n=20):\n",
    "    \"\"\"Predict probabilities for locations without Aurora shops\"\"\"\n",
    "    # Filter locations without shops\n",
    "    no_shop_mask = (df['has_aurora'] == 0) & (df['boundary_effect'] == 0)\n",
    "    df_potential = df[no_shop_mask].copy()\n",
    "    \n",
    "    if len(df_potential) == 0:\n",
    "        print(\"No potential locations found\")\n",
    "        return None\n",
    "    \n",
    "    # Prepare features\n",
    "    X_potential = df_potential[feature_cols].fillna(0)\n",
    "    \n",
    "    # Predict probabilities\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        probabilities = model.predict_proba(X_potential)[:, 1]\n",
    "    else:\n",
    "        # For models without predict_proba, use decision function\n",
    "        probabilities = model.decision_function(X_potential)\n",
    "        # Normalize to [0, 1]\n",
    "        probabilities = (probabilities - probabilities.min()) / (probabilities.max() - probabilities.min())\n",
    "    \n",
    "    df_potential['aurora_probability'] = probabilities\n",
    "    \n",
    "    # Get top locations\n",
    "    top_locations = df_potential.nlargest(top_n, 'aurora_probability')[['cell', 'aurora_probability'] + feature_cols[:10]]\n",
    "    \n",
    "    print(f\"\\nTop {top_n} recommended locations for new Aurora shops:\")\n",
    "    print(\"=\"*60)\n",
    "    for idx, row in top_locations.iterrows():\n",
    "        print(f\"Cell: {row['cell']}, Probability: {row['aurora_probability']:.3f}\")\n",
    "    \n",
    "    return top_locations\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    print(\"Aurora Shop Location Prediction Model\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Load data\n",
    "    print(\"\\n1. Loading data...\")\n",
    "    df_kyiv, df_lviv = load_data(conn_string)\n",
    "    \n",
    "    # Explore data\n",
    "    print(\"\\n2. Exploring data...\")\n",
    "    explore_data(df_kyiv, \"Kyiv\")\n",
    "    explore_data(df_lviv, \"Lviv\")\n",
    "    \n",
    "    # Prepare features\n",
    "    print(\"\\n3. Preparing features...\")\n",
    "    X_kyiv, y_kyiv, feature_cols = prepare_features(df_kyiv, FEATURE_COLUMNS)\n",
    "    X_lviv, y_lviv, _ = prepare_features(df_lviv, FEATURE_COLUMNS)\n",
    "    \n",
    "    # Split Kyiv data for training/validation\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_kyiv, y_kyiv, test_size=0.2, random_state=42, stratify=y_kyiv\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTraining set size: {len(X_train)}\")\n",
    "    print(f\"Validation set size: {len(X_val)}\")\n",
    "    print(f\"Test set (Lviv) size: {len(X_lviv)}\")\n",
    "    \n",
    "    # Train models\n",
    "    print(\"\\n4. Training models on Kyiv data...\")\n",
    "    results_val = train_models(X_train, y_train, X_val, y_val)\n",
    "    \n",
    "    # Plot comparison\n",
    "    print(\"\\n5. Comparing model performance...\")\n",
    "    plot_model_comparison(results_val)\n",
    "    \n",
    "    # Select best model\n",
    "    best_model_name = max(results_val.items(), key=lambda x: x[1]['f1'])[0]\n",
    "    print(f\"\\nBest model (by F1-score): {best_model_name}\")\n",
    "    \n",
    "    # Retrain best model on full Kyiv data\n",
    "    best_model_type = type(results_val[best_model_name]['model'])\n",
    "    if best_model_name == 'Random Forest':\n",
    "        best_model = RandomForestClassifier(n_estimators=200, random_state=42, class_weight='balanced')\n",
    "    elif best_model_name == 'XGBoost':\n",
    "        best_model = xgb.XGBClassifier(n_estimators=200, random_state=42, scale_pos_weight=len(y_kyiv)/y_kyiv.sum())\n",
    "    else:\n",
    "        best_model = results_val[best_model_name]['model']\n",
    "    \n",
    "    # Standardize if necessary\n",
    "    if best_model_name in ['Logistic Regression', 'SVM', 'KNN']:\n",
    "        scaler = StandardScaler()\n",
    "        X_kyiv_scaled = scaler.fit_transform(X_kyiv)\n",
    "        X_lviv_scaled = scaler.transform(X_lviv)\n",
    "        best_model.fit(X_kyiv_scaled, y_kyiv)\n",
    "        X_lviv_final = X_lviv_scaled\n",
    "    else:\n",
    "        best_model.fit(X_kyiv, y_kyiv)\n",
    "        X_lviv_final = X_lviv\n",
    "    \n",
    "    # Evaluate on Lviv\n",
    "    print(f\"\\n6. Evaluating {best_model_name} on Lviv test data...\")\n",
    "    y_pred_lviv = best_model.predict(X_lviv_final)\n",
    "    y_pred_proba_lviv = best_model.predict_proba(X_lviv_final)[:, 1]\n",
    "    \n",
    "    print(\"\\nTest Results (Lviv):\")\n",
    "    print(classification_report(y_lviv, y_pred_lviv))\n",
    "    \n",
    "    # Feature importance\n",
    "    print(\"\\n7. Analyzing feature importance...\")\n",
    "    analyze_feature_importance(best_model, feature_cols)\n",
    "    \n",
    "    # Predict new locations\n",
    "    print(\"\\n8. Predicting best locations for new shops...\")\n",
    "    print(\"\\nKyiv recommendations:\")\n",
    "    predict_new_locations(best_model, df_kyiv, feature_cols)\n",
    "    \n",
    "    print(\"\\nLviv recommendations:\")\n",
    "    predict_new_locations(best_model, df_lviv, feature_cols)\n",
    "    \n",
    "    # Save model\n",
    "    print(\"\\n9. Saving model...\")\n",
    "    import joblib\n",
    "    joblib.dump(best_model, 'aurora_location_model.pkl')\n",
    "    if best_model_name in ['Logistic Regression', 'SVM', 'KNN']:\n",
    "        joblib.dump(scaler, 'aurora_location_scaler.pkl')\n",
    "    print(\"Model saved successfully!\")\n",
    "    \n",
    "    return best_model, results_val\n",
    "\n",
    "model, results = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "osm-postgis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
